{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43c08618-4299-41e0-9329-dab3a99e1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ea02549-3ccf-4d44-9a6f-47c9c938a0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82a11216-9182-413b-9bad-6aabfddff2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0754fdaa-08a2-4bcb-aedc-2bd9321fcc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def tokenize_data(df, tokenizer):\n",
    "    return tokenizer(df[\"Text\"].tolist(), padding=True, truncation=True, max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ff5ff4b2-3e28-42e7-93ac-5c1e9c5b2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./../../data/binary_class_sentences.csv\"\n",
    "df = pd.read_csv(file_path, header=None, names=['Text', 'Label'])\n",
    "df['Label'] = df['Label'].astype(int)\n",
    "train_dataset_df, eval_dataset_df = train_test_split(df, shuffle=True)\n",
    "train_dataset_df = train_dataset_df.reset_index(drop=True)\n",
    "eval_dataset_df = eval_dataset_df.reset_index(drop=True)\n",
    "\n",
    "# Grab labels\n",
    "train_labels = train_dataset_df[\"Label\"].tolist()\n",
    "eval_labels = eval_dataset_df[\"Label\"].tolist()\n",
    "\n",
    "# Tokenize data\n",
    "tokenized_train_dataset = tokenize_data(train_dataset_df, tokenizer)\n",
    "tokenized_eval_dataset = tokenize_data(eval_dataset_df, tokenizer)\n",
    "\n",
    "# Apply labels to tokenized data\n",
    "train_dataset = CustomDataset(tokenized_train_dataset, train_labels)\n",
    "eval_dataset = CustomDataset(tokenized_eval_dataset, eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7815e89c-467b-4d32-a86d-dfe8af1f346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_ids for one example: torch.Size([84])\n",
      "Shape of attention_mask for one example: torch.Size([84])\n",
      "Label for one example: tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alext\\AppData\\Local\\Temp\\ipykernel_5748\\1502448823.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "example = train_dataset[0]\n",
    "print(\"Shape of input_ids for one example:\", example['input_ids'].shape)\n",
    "print(\"Shape of attention_mask for one example:\", example['attention_mask'].shape)\n",
    "print(\"Label for one example:\", example['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0d65b7d-cafa-4179-a80e-d320935b4016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2987\n",
      "996\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c146991b-2440-425b-9bfd-8cba727873d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer=Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5cdbce-f509-4351-9494-3032c915d436",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee489499-3dfb-49da-bff2-109bd4e087e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alext\\AppData\\Local\\Temp\\ipykernel_5748\\1502448823.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12166711688041687, 'eval_runtime': 105.4452, 'eval_samples_per_second': 9.446, 'eval_steps_per_second': 0.152, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate(eval_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "47891941-18e0-4d7b-ab03-f24624e46b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alext\\AppData\\Local\\Temp\\ipykernel_5748\\1502448823.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.704887   2.886109 ]\n",
      " [ 2.2272937 -2.7609262]\n",
      " [ 2.0492885 -2.5689569]\n",
      " ...\n",
      " [ 2.3261707 -2.7602344]\n",
      " [ 2.353644  -2.8311214]\n",
      " [ 2.4253397 -2.9407072]]\n",
      "[1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1 1\n",
      " 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1\n",
      " 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0\n",
      " 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0\n",
      " 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1\n",
      " 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0\n",
      " 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0\n",
      " 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0\n",
      " 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1\n",
      " 1 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0\n",
      " 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 1 1\n",
      " 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1\n",
      " 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 1 1\n",
      " 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1\n",
      " 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1\n",
      " 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0\n",
      " 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
      " 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0\n",
      " 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1\n",
      " 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0\n",
      " 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0\n",
      " 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "prediction_output = trainer.predict(eval_dataset)\n",
    "logits = torch.tensor(prediction_output.predictions)\n",
    "probabilities = F.softmax(logits, dim=1).numpy()\n",
    "predicted_classes = np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "97bb1f11-f283-4e76-a075-ecf74ebe3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in eval_dataset_df.iterrows():\n",
    "    sentence = row['Text']  # Replace 'Text' with the name of your column containing the sentences\n",
    "    print(i)\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Predicted Class: {predicted_classes[i]}\")\n",
    "    print(f\"Probability of Class 0: {probabilities[i][0]}\")\n",
    "    print(f\"Probability of Class 1: {probabilities[i][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c3f6c-167e-43df-b7d4-3a2ef533204b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
